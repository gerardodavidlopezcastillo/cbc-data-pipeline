# üöÄ CBC Mobility Data Pipeline

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)
[![Docker](https://img.shields.io/badge/Docker-Enabled-blue.svg)](https://www.docker.com/)
[![Terraform](https://img.shields.io/badge/IaC-Terraform-purple.svg)](https://www.terraform.io/)
[![Spark](https://img.shields.io/badge/Apache-Spark-orange.svg)](https://spark.apache.org/)
![AWS](https://img.shields.io/badge/AWS-Hybrid_Architecture-FF9900)
[![Status](https://img.shields.io/badge/Status-Complete-green.svg)]()

> **Prueba T√©cnica** > Se realizo soluci√≥n robusta y escalable para el procesamiento de datos.

---
## üìã Resumen
> [!NOTE]
> Este proyecto fue solicitado inicialmente para resolver un desaf√≠o de ingesta y transformaci√≥n de datos locales. Sin embargo, ya que dentro de uno de los incisos se dio la puerta para agregarle cosas extra a la prueba se realizo una **arquitectura productiva moderna**, la prueba se realizo para soportar un enfoque h√≠brido:

1.  **Modo On-Premise: üñ•Ô∏è** Ejecuci√≥n local r√°pida para desarrollo y debugging.
2.  **Modo Cloud (AWS): ‚òÅÔ∏è** Despliegue contenerizado.

La arquitectura en la nube se aprovision√≥ al 100% con Terraform y conecta los siguientes componentes:

* ** üîÑ C√≥mputo:** **ECS Fargate** recibe la orden, descarga la imagen Docker desde **ECR** (Elastic Container Registry) y levanta un contenedor ef√≠mero.

* ** üîí Seguridad:** El contenedor asume un **IAM Task Role** espec√≠fico que le otorga permisos para leer y escribir en S3, sin usar claves est√°ticas.

* ** üíæ Almacenamiento (Data Lake):**
    * **S3 Bronze (Raw):** Origen de datos crudos.
    * **S3 Silver (Processed):** Destino de datos limpios en formato Parquet particionado.
     * **Athena:** Consulta a los archivos en S3

* ** üë§ Gobierno y Consumo:**
    * **AWS Glue Catalog:** Mapea los esquemas de los archivos Parquet en S3.
    * **Redshift Serverless:** Permite realizar consultas SQL anal√≠ticas sobre los datos en S3 (Silver) utilizando **Redshift Spectrum** y la metadata de Glue.

El objetivo fue demostrar no solo la capacidad de transformar datos, sino de construir el **ecosistema completo** necesario para operar pipelines de datos con calidad, seguridad y mantenibilidad.

---

## üíª ‚å®Ô∏è Est√°ndares de C√≥digo y Buenas Pr√°cticas Aplicadas

> [!INFO]
> Este proyecto fue desarrollado siguiendo est√°ndares de ingenier√≠a de software y data engineering utilizados en entornos corporativos y equipos senior.

### üöß Creaci√≥n del entorno de desarrollo

Para garantizar el aislamiento de dependencias y la consistencia del entorno de ejecuci√≥n, el proyecto utiliza un entorno virtual administrado con **Conda**.

El entorno fue creado utilizando Python 3.10 con el siguiente comando:

```bash
conda create -n prjct_de_cbc python=3.10 -y
```

![Entorno conda](docs/images/activate_project_cbc.png)


### üìú Estilo de c√≥digo Python
- **Formateo autom√°tico con Black** (alineado a PEP-8).
- **Imports ordenados** (est√°ndar: librer√≠as est√°ndar ‚Üí terceros ‚Üí m√≥dulos locales).
- **Longitud de l√≠nea controlada** (70 / 89 / 100 seg√∫n contexto y legibilidad).
- **Indentaci√≥n consistente** (4 espacios).
- **Nombres de variables y funciones en `snake_case`.**
- **Clases en `CamelCase`.**
- **Constantes en `UPPER_CASE`.**
- Uso correcto de **espacios, par√©ntesis y method chaining** para legibilidad.

### üìÑ Tipado y documentaci√≥n
- **Type hints** para indicar claramente:
  - Qu√© entra a una funci√≥n
  - Qu√© devuelve
  - *(No afectan la ejecuci√≥n, pero mejoran mantenibilidad y tooling)*
- **Docstrings** para explicar:
  - Qu√© hace la funci√≥n
  - Por qu√© existe
  - Reglas de negocio relevantes
  - *(No afectan la ejecuci√≥n, pero documentan intenci√≥n)*

### üóùÔ∏è Constantes y valores por defecto
- **Constantes declaradas en MAY√öSCULAS**.
- **Valores hardcodeados centralizados** en `src/constants.py`.
- Eliminaci√≥n de *Magic Strings* dispersos en el c√≥digo.
- Uso expl√≠cito de valores dummy controlados (ej. `PD`, fechas por defecto).

### Manejo de fechas y tiempo
- **Timestamps manejados con buenas practivas, siempre en UTC (UTC+0)**.
- Formatos de fecha estandarizados y centralizados.


---

## üèó Arquitectura

La soluci√≥n implementa una arquitectura desacoplada donde la l√≥gica de negocio (ETL Spark) es independiente de la infraestructura subyacente.
Se implemento el framework brindado por AWS SDLF (Serverless Data Lake Framework)

* Arquitectura dise√±ada en la nube

![Data Lake](docs/images/dl-analytics.png)


> [!INFO]
> Pr√°cticas altamente necesarias en proyectos corporativos de grandes vol√∫menes de datos batch/streaming

---

## üß† Principios de dise√±o

*Estas decisiones buscan maximizar mantenibilidad, claridad y escalabilidad sin sobre-ingenier√≠a.*

- **ETL cl√°sico bien definido**: Extract ‚Üí Transform ‚Üí Load
- **Configuraci√≥n desacoplada** (YAML + OmegaConf)
- **C√≥digo testeable y legible**
- **No sobre‚Äëingenier√≠a** (decisi√≥n consciente por ser prueba t√©cnica)
- **Preparado para escalar** a un entorno corporativo real

---

## üìÅ Estructura general del proyecto `cbc-data-pipeline`

El proyecto sigue una arquitectura modular inspirada en los principios de **Cookiecutter Data Science** y **Kedro**, adaptada para soportar despliegues h√≠bridos (Local/Cloud).

```text
CBC/
‚îú‚îÄ‚îÄ conf/                       # Gesti√≥n de Configuraci√≥n (OmegaConf)
‚îÇ   ‚îú‚îÄ‚îÄ base/                   # Par√°metros compartidos (schemas, constantes)
‚îÇ   ‚îú‚îÄ‚îÄ cloud/                  # Config espec√≠fica para AWS (S3 paths)
‚îÇ   ‚îî‚îÄ‚îÄ onpremise/              # Config para ejecuci√≥n local (Local FS paths)
‚îú‚îÄ‚îÄ data/                       # Capa de datos
‚îÇ   ‚îú‚îÄ‚îÄ input/                  # Datos crudos (Raw/Bronze)
‚îÇ   ‚îî‚îÄ‚îÄ processed/              # Datos transformados (Silver/Parquet)
‚îú‚îÄ‚îÄ infrastructure/             # Esta carpeta NO va dentro del ETL en un escenario prod
‚îÇ   ‚îú‚îÄ‚îÄ docker/                 # Definici√≥n de Container (Multi-arch)
‚îÇ   ‚îú‚îÄ‚îÄ sdlf-dataset/           # CloudFormation para Gobierno de Datos (Glue/LakeFormation)
‚îÇ   ‚îî‚îÄ‚îÄ terraform/              # Provisionamiento de Compute & Network (ECS, ECR, IAM, Redshift, Glue, S3)
‚îú‚îÄ‚îÄ notebook/                   # Zona de Experimentaci√≥n (Sandboxing)
‚îÇ   ‚îî‚îÄ‚îÄ exploratory.ipynb       # Pruebas unitarias
‚îú‚îÄ‚îÄ src/                        # C√≥digo Fuente de la Aplicaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ analytics_helpers.py    # L√≥gica de negocio pura, se agrega archivo en blanco
‚îÇ   ‚îú‚îÄ‚îÄ analytics_utils.py      # Utilidades de IO y Spark
‚îÇ   ‚îú‚îÄ‚îÄ constants.py            # Constantes globales importantes en proyectos grandes
‚îÇ   ‚îî‚îÄ‚îÄ etl.py                  # Clase orquestadora del Pipeline
‚îú‚îÄ‚îÄ test/                       # Aseguramiento de Calidad (QA)
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py             # Separamos logica de folderes
‚îÇ   ‚îî‚îÄ‚îÄ test_cleaning.py        # Pruebas unitaras para transformaciones
‚îú‚îÄ‚îÄ main.py                     # Entrypoint de la aplicaci√≥n
‚îú‚îÄ‚îÄ requirements.txt            # Dependencias Python (pinned versions)
‚îî‚îÄ‚îÄ README.md                   # Documentaci√≥n t√©cnica
```

---

## ‚öôÔ∏è Configuraci√≥n (YAML)

### `conf/base/parameters.yaml`
Contiene **reglas de negocio** y par√°metros del pipeline:

- Pa√≠ses v√°lidos `GT`
- Rango de fechas 
- Unidades de medida `CS: 20`
- Tipos de entrega  `["ZPRE", "ZVE1"]`
- Columnas relevantes

üëâ Este archivo representa la **configuraci√≥n funcional del negocio**.

---

### `conf/cloud/env.yaml`
Configuraci√≥n espec√≠fica para ejecuci√≥n en la nube:

- Rutas S3 (input/output) `s3a://cbc-datalake-bronze/mobility/global_mobility_data_entrega_productos.csv`
- Formato de salida (Parquet) `s3a://cbc-datalake-silver/mobility/processed/x.parquet`
- Particiones Spark `shuffle_partitions : 4` 
- Par√°metros de ejecuci√≥n distribuida

---

### `conf/onpremise/env.yaml`
Configuraci√≥n para ejecuci√≥n local:

- Rutas locales `data/input/global_mobility_data_entrega_productos.csv`
- N√∫mero de particiones Spark `shuffle_partitions : 1` 
- Formato de salida

---

## üìä Datos

### Entrada

`data/input/global_mobility_data_entrega_productos.csv`

Archivo CSV de entrada utilizado para la prueba t√©cnica.


### Salida

`data/processed/`

- Cloud o Onpremise
- Datos transformados en formato **Parquet**
- Particionados por `fecha_particion`

---

## üèóÔ∏è Infraestructura

### Docker

`docker/Dockerfile`

- Python **3.10**
- OpenJDK **11**
- PySpark
- Usuario **no root** (buenas pr√°cticas de seguridad)

Permite ejecuci√≥n reproducible del pipeline.

* Docker Imagen

![Docker images](docs/images/docker_images.png)

* Docker Contenedores

![Docker containers](docs/images/docker_containers.png)

---

### üèóÔ∏è Terraform

`terraform/main.tf`

Infraestructura base en AWS:

- Buckets S3 **Bronze / Silver**
- ECR
- ECS
- Redshift
- Glue Catalog
- Glue Crawler
- ECR Repository

üëâ Incluido como referencia de **arquitectura cloud**, no como despliegue obligatorio para la prueba.

* Terraform init

![Terraform init](docs/images/terraform_init.png)

* Terraform plan

![Terraform init](docs/images/terraform_plan.png)

* Terraform apply

![Terraform init](docs/images/terraform_apply.png)

---

### üìñ CloudFormation (SDLF)

`infrastructure/sdlf-dataset/`

Implementaci√≥n de gobierno de datos y esquemas (Schema-as-Code):

- **Glue Database** (`cbcmobility`)
- **Glue External Table** (`st_cbc_test`) con esquema tipado
- **Lake Formation** para gesti√≥n de permisos
- **Nested Stacks** para arquitectura modular

üëâ Desacopla la definici√≥n de metadatos de la infraestructura de c√≥mputo.
Sirve para definir y versionar las bases de datos y tablas Glue de forma declarativa usando CloudFormation/YAML.

---

### üìì Notebook Exploratorio

`notebook/exploratory.ipynb`

Entorno interactivo para validaci√≥n y debugging del pipeline:

- Importa la clase `ETLEngineer` directamente desde `src/` (evita duplicidad de c√≥digo).
- Carga y combina configuraciones din√°micamente con **OmegaConf**.
- Ejecuta y visualiza paso a paso las fases **Extract, Transform y Load**.

üëâ Permite inspeccionar los DataFrames y esquemas intermedios antes de la ejecuci√≥n productiva.

* Notebook exploratorio
Se pueden visualizar los cambios que van sufriendo los datos conforme a los metodos/funciones.
![Notebook exploratorio](docs/images/notebook_exploratorio.png)

---

## üß© CODIGO FUENTE (`src/`)

- Separaci√≥n conceptual **MECE**:
- No se sobre‚Äëforz√≥ la abstracci√≥n por tratarse de un pipeline compacto.
- Uso expl√≠cito de `None` en funciones donde aplica para claridad sem√°ntica.

### `__init__.py`

Archivo vac√≠o para declarar `src` como paquete Python.

---

### `analytics_helpers.py`

Archivo **placeholder / documental**.

En un entorno corporativo contendr√≠a:

- M√©tricas
- Agregaciones
- KPIs

Se deja intencionalmente simple para la prueba t√©cnica.

---

### `analytics_utils.py`

> [!TIP]
> Este m√≥dulo est√° dise√±ado para ser reutilizable entre distintos pipelines, evitando dependencias de columnas espec√≠ficas o reglas hardcodeadas.
En un ambiente real, seria una dependencia `fuera del ETL` para uso del departamento.

Utilidades gen√©ricas para PySpark.

Funcionalidad principal:

- `cleaning_df_spark(df)`
  - Limpieza basada en **tipos de datos**, no en columnas hardcodeadas
  - Manejo de nulos para:
    - Strings
    - Num√©ricos
    - Fechas

üëâ Dise√±ado para ser **reutilizable y testeable**.

---

### `constants.py`

> [!IMPORTANT]
> No todas las variables fueron llevadas a constantes intencionalmente. En una prueba t√©cnica peque√±a se evita **sobre‚Äëingenier√≠a** innecesaria, priorizando claridad y pragmatismo.

Centraliza constantes del proyecto:

- Valores por defecto
- Formatos de fecha

Se evita mover todo a constantes para **no sobre‚Äëingenierizar** la prueba.

---

### `etl.py`

Contiene la clase **`ETLEngineer`**, motor principal del pipeline.

#### üö© M√©todos

- `read_data()`
  - Lectura desde CSV local o S3

- `transform_data()`
  - Filtrado por pa√≠s y rango de fechas
  - Normalizaci√≥n de unidades
  - Clasificaci√≥n de tipo de entrega
  - Limpieza gen√©rica (`AnalyticsUtils`)
  - Eliminaci√≥n de duplicados

- `write_data()`
  - Renombrado de columnas
  - Escritura Parquet particionada por `fecha_particion`

- `run()`
  - Orquesta el flujo completo
  - Manejo de excepciones y logging

---

### `main.py`

**Entrypoint del proyecto**.

Responsabilidades:

- Configuraci√≥n de logging
- Carga de configuraci√≥n en cascada:
  - Base ‚Üí Entorno ‚Üí CLI
- Instanciaci√≥n de `ETLEngineer`
- Ejecuci√≥n del pipeline

---

## üß™ Tests

> [!NOTE]
> Los tests no buscan validar Spark como framework, sino garantizar que **nuestra l√≥gica de negocio** se comporta correctamente ante distintos escenarios de datos.

- Pruebas automatizadas con **pytest**.
- Uso de `assert` para validar reglas cr√≠ticas de negocio.
- Enfoque en testear l√≥gica propia, no internals de Spark.

La carpeta `test/` contiene ** 1 tests unitario** enfocados en:

- Validar l√≥gica de negocio
- Probar utilidades propias
- Evitar testear Spark como framework

üëâ Se prioriza **calidad sobre cantidad** de tests.

* Prueba unitaria

![Prueba unitaria](docs/images/prueba_unitaria.png)

---

## üì¶ Gesti√≥n de dependencias

El proyecto incluye un archivo `requirements.txt` que define todas las dependencias necesarias para su correcta ejecuci√≥n.

Este archivo fue generado a partir del entorno virtual activo utilizando el siguiente comando:

```bash
pip list --format=freeze > requirements.txt
```

Incluye:

- PySpark 3.5
- OmegaConf
- Pandas
- NumPy
- Pytest
- Jupyter
- Librer√≠as auxiliares para debugging

---

## ‚ñ∂Ô∏è Ejecuci√≥n

**Ejecuci√≥n onpremise**
```bash
(prjct_de_cbc) GDLC-LAPTOP:~ glopez$ python main.py
```

![Ejecucion local](docs/images/ejecucion_local.png)


**Ejecuci√≥n parametrizada**
```bash
(prjct_de_cbc) GDLC-LAPTOP:~ glopez$ python main.py \
pipeline.start_date=20250101 \
pipeline.end_date=20251231 \
pipeline.write_mode=overwrite \
pipeline.country='SV'
```

![Ejecucion local parametrizada](docs/images/ejecucion_local_parametrizada.png)

**Ejecuci√≥n docker local**
```bash
(prjct_de_cbc) GDLC-LAPTOP:~ glopez$ eval $(aws configure export-credentials --profile gdlopezcastillo-cbc --format env)

(prjct_de_cbc) GDLC-LAPTOP:~ glopez$ docker run --platform linux/amd64 \
  -e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
  -e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
  -e AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \
  -e AWS_REGION=us-east-1 \
  -e ENV=CLOUD \
  508186271604.dkr.ecr.us-east-1.amazonaws.com/cbc-data-pipeline:latest
```

![Ejecucion docker local](docs/images/ejecucion_docker_local.png)

**Ejecuci√≥n AWS**

* IAM

![AWS IAM](docs/images/aws_iam.png)

* ECR

![AWS ECR](docs/images/aws_ecr.png)

* ECS

![AWS ECS](docs/images/aws_ecs.png)

* CloudWatch

Log que vemos cuando se ejecuta el proceso local
![AWS CloudWatch](docs/images/aws_cloudwatch.png)

* S3

![AWS S3](docs/images/aws_s3.png)

* Athena

Resultados de la tabla en silver anteponiendo st (staging)
![AWS Athena](docs/images/aws_athena.png)

* Redshift

![AWS Redshift](docs/images/aws_redshift.png)

---

## üìö Propuesta de columnas en tabla

* Est√°ndar snake_case: Todos los atributos deben persistirse en min√∫sculas separadas por guiones bajos.
* Expresividad Sem√°ntica: Se priorizan nombres descriptivos sobre abreviaturas cr√≠pticas.
* Trazabilidad para todo proceso de ingesta de archivos (Excel, CSV, Flat Files), se inyectan obligatoriamente columnas de metadatos t√©cnicos al inicio del esquema:
* El esquema de particionado se define en funci√≥n del volumen de datos para evitar el problema de "Small Files" (exceso de archivos peque√±os que degradan el rendimiento de lectura en S3). Para este caso pedian en prueba particionar por fecha_proceso -> fecha_particion


[!TIP] Para el dataset final (Silver/Gold), se adopt√≥ una taxonom√≠a basada en prefijos sem√°nticos. Esto facilita el autoservicio (self-service BI) permitiendo a los analistas identificar la naturaleza del dato (dimensi√≥n, m√©trica, fecha o bandera) sin consultar el esquema t√©cnico.

### üëΩ Taxonom√≠a de Prefijos (Naming Convention)

| Prefijo | Significado | Tipo de Dato Recomendado | Ejemplo |
| :--- | :--- | :--- | :--- |
| `id_` | Identificador √∫nico o llave for√°nea | `INT` / `BIGINT` | `id_ruta` |
| `cod_` | C√≥digo de negocio alfanum√©rico | `STRING` | `cod_sku_material` |
| `desc_` | Descripci√≥n textual o atributo dimensional | `STRING` | `desc_pais` |
| `dtm_` | Fecha y hora exacta (Date Time) | `TIMESTAMP` | `dtm_fecha_carga` |
| `num_` | Conteo de unidades discretas (enteros) | `INT` | `num_unidades` |
| `vlr_` | Valor monetario | `DOUBLE` / `DECIMAL` | `vlr_precio` |
| `es_` | Bandera booleana (Estado/Flag) | `INT (0/1)` | `es_entrega_rutina` |

### üéâ Definici√≥n del Esquema (Schema Definition)
La tabla final st_cbc_test (Silver Layer) queda definida con la siguiente metadata t√©cnica y de negocio:

| Columna F√≠sica | Tipo (Hive/Spark) | Descripci√≥n Funcional | Origen / Transformaci√≥n |
| :--- | :--- | :--- | :--- |
| `desc_fuente` | `string` | Sistema origen de la informaci√≥n. | Inyectado por ETL (ej: 'SAP', 'CSV_Manual') |
| `desc_nombre_archivo` | `string` | Trazabilidad del archivo origen (Lineage). | `input_file_name()` |
| `dtm_fecha_carga` | `timestamp` | Momento exacto de ingesti√≥n al Data Lake. | `current_timestamp()` |
| `desc_pais` | `string` | Pa√≠s de operaci√≥n (ISO Code). | Columna `pais` normalizada |
| `id_transporte` | `int` | Identificador √∫nico del veh√≠culo. | Cast a `Integer` |
| `id_ruta` | `int` | Llave de la ruta de distribuci√≥n. | Cast a `Integer` |
| `cod_sku_material` | `string` | C√≥digo SKU del producto (Material). | Limpieza de espacios (Trim) |
| `desc_unidad` | `string` | Unidad de medida estandarizada. | Mapeo `UNITS_CONVERSION` (ej: CS, ST) |
| `desc_tipo_entrega` | `string` | Categor√≠a log√≠stica de la entrega. | Regla de negocio (`ZPRE`, `ZVE1`) |
| `num_unidades` | `int` | Cantidad f√≠sica de unidades. | Campo directo |
| `cantidad` | `double` | Cantidad matem√°tica base. | Campo directo |
| `cantidad_unidades_totales`| `double` | Total acumulado calculado. | `cantidad * factor_conversion` |
| `vlr_precio` | `double` | Precio unitario o valor total. | Campo directo |
| `es_entrega_rutina` | `int` | **1** si es entrega est√°ndar, **0** si no. | Derivado de `tipo_entrega` |
| `es_entrega_bonificacion`| `int` | **1** si es bonificaci√≥n, **0** si no. | Derivado de `tipo_entrega` |
| `fecha_particion` | `bigint` | Llave de partici√≥n f√≠sica en S3. | Formato `YYYYMMDD` (ej: `20250101`) |

---

### üöë Control de versiones en GIT
- Uso de **Git con convenci√≥n de ramas**:
  - `feature/*` (para nuevas funcionalidades) Desarrollo de nuevas capacidades
  - `bugfix/*` (para correcciones) Correcciones de errores no cr√≠ticos.
  - `hotfix/*` (para arreglos urgentes) Parches urgentes a producci√≥n
  - `main`: Rama productiva estable. Protegida contra escrituras directas
  - `develop`: Rama de integraci√≥n
  - `.gitignore` Por ser un entorno/preuba, no se agrega nada en gitignore, pero es nesario saber que se tiene que excluir:
    - Archivos de variables de entorno (.env, .env.local)
    - Llaves de acceso y credenciales (*.pem, credentials.json)
    - Configuraciones locales de IDE (.vscode/, .idea/).

---

<div align="center">
  <h3><b>Gerardo L√≥pez</b></h3>
  <p>Senior Data Engineer | Data Architect | Cloud Engineer</p>

<div align="center">
  <a href="https://linktr.ee/gdlopezcastillo" target="_blank">
    <img src="https://img.shields.io/badge/Linktree-43E660?style=for-the-badge&logo=linktree&logoColor=white" alt="Linktree" />
  </a>
  <a href="https://www.linkedin.com/in/gdlopezcastillo/" target="_blank">
    <img src="https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white" alt="LinkedIn" />
  </a>
  <a href="mailto:gdlopezcastillo@gmail.com">
    <img src="https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white" alt="Gmail" />
  </a>
  <a href="https://github.com/gerardodavidlopezcastillo" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white" alt="GitHub" />
  </a>
</div>